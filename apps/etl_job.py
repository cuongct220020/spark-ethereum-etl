from pyspark.sql import SparkSession
import os

# Táº¡o Spark Session
spark = SparkSession.builder \
    .appName("Ethereum ETL Job") \
    .getOrCreate()

print("âœ… Spark Session created successfully. Running Job...")

# Láº¥y Ä‘Æ°á»ng dáº«n tuyá»‡t Ä‘á»‘i
base_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
# Äá»c CSV file (sá»­ dá»¥ng Ä‘Æ°á»ng dáº«n tuyá»‡t Ä‘á»‘i)
csv_file = os.path.join(project_root, "data", "blocks_20659158_20659258.csv")

print(f"ğŸ“‚ Äang Ä‘á»c file: {csv_file}")

try:
    df = spark.read.csv(csv_file, header=True, inferSchema=True)

    print(f"âœ… Äá»c file thÃ nh cÃ´ng!")
    print(f"ğŸ“Š Sá»‘ lÆ°á»£ng blocks: {df.count()}")
    print(f"ğŸ“‹ Schema:")
    df.printSchema()

    print("ğŸ” Sample data:")
    df.show(5)

    # Thá»±c hiá»‡n transformations
    print("\nğŸ“ˆ Thá»‘ng kÃª cÆ¡ báº£n:")
    df.describe().show()

    # LÆ°u káº¿t quáº£
    output_path = os.path.join(project_root, "data", "processed_blocks")
    print(f"\nğŸ’¾ LÆ°u káº¿t quáº£ vÃ o: {output_path}")

    df.write.mode("overwrite").parquet(output_path)

    print("âœ… Job hoÃ n thÃ nh thÃ nh cÃ´ng!")

except Exception as e:
    print(f"âŒ Lá»—i: {e}")
    raise

finally:
    spark.stop()
    print("ğŸ›‘ Spark Session Ä‘Ã£ dá»«ng.")